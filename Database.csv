JournalName,PaperTittle,Year,Country,Continent,DataSet,DataSetType,ResearchProblem ,ResearchObjective,ImplementationFramework,ArchitectureProperties,BaselineModels,BestModels,Methodology,EvaluationMechanism,EvaluationMetric
Association for the Advancement of Artificial Intelligence,A Memory-Network Based Solution for Multivariate Time-Series Forecasting,2018,Taiwan,Asia,"univariate and multivariate sequential datasets  (weather, traffic, electricity, financial market, power cosnumption)",sequential,"Complexity in modelling and capturing extremely long-term sequential patterns using traditional frameworks such as Recurrent Neural Network (RNN). Furthermore, lack of explainability within the implementation of deep learning models.","To propose an explainable sequential forecasting model (Memory Time series network (MTNet)) composed of a large memory component, three separate encoders, an autoregressive component and an attention mechanism.","8 models were designed namely autogressive model (AR), autoregressive model with linear least squares loss function (LRidge), autoregressive model with Support Vector (LSVR), Gaussian Process time series model (GP), Multilayer Perception (MLP) and autoregressive model (VAR) (VAR-MLP), recurrent neural network model using GRU cell (RNN-GRU), dual-stage attention based recurrent neural network (DA-RNN), Long- and Short-term Time-series Network (LSTNet) and Mnet","The MTNet model utilized recurrent neural network features, convolutional filters and attention mechanism based on Adam optimizer. The MTNet framework had a memory component (for storing the long-term historical data), three different embedding feature maps generated by three independent encoders (to convert the input data and memory data to their feature representations), and an autoregressive component.","Autoregressive model (AR), autoregressive model with linear least squares loss function (LRidge), autoregressive model with Support Vector (LSVR), Gaussian Process time series model (GP), Multilayer Perception (MLP) and autoregressive model (VAR) (VAR-MLP), recurrent neural network model using GRU cell (RNN-GRU), dual-stage attention based recurrent neural network (DA-RNN), Long- and Short-term Time-series Network (LSTNet) and Mnet",Memory Time series network (MTNet),Comparative experimental approach,Prediction accuracy,Root mean squared error (RMSE) and mean absolute error (MAE) for univariate task and root relative squared error (RRSE) and empirical correlation coefficient (CORR) for multivariate task.
"AAAI Conference on Artificial Intelligence (AAAI), 2020.",Particle Filter Recurrent Neural Networks,2019,Singapore,Asia,"sequence datasets from multiple domains for both regression and classification tasks for text classification, activity recognition, stock price prediction, robot localization.",Sequence,Sequential data prediction problem associated with highly variable and noisy real-world data.  ,"To propose a particle filter recurrent neural networks (PF-RNNs) prediction framework which reduce the data required for learning on various noisy sequence prediction tasks such as text classification, activity recognition, stock price prediction, robot localization",particle filter recurrent neural networks (PF-RNNs),Hyperparameter tuning= grid search,"PF-LSTM, PF-GRU with LSTM and GRU",particle filter recurrent neural networks (PF-RNNs),Experimental simulation,Prediction accuracy,Mean Square Error (MSE)
32nd Conference on Neural Information Processing Systems (NIPS 2018),Benchmarking Deep Sequential Models on Volatility Predictions for Financial Time Series ,2018,Canada,North America,Financial stock price,Sequential ,Volatility prediction modelling problem,Studying the performance of latest deep sequential models in solving volatility modelling problem in financial time series. Compared accuracy performance of 11 state of the art deep sequential models on volatility predictions.,"Dilated temporal convolutional network (TCN), dilated recurrent neural network (DilatedRNN), independently recurrent neural network (IndRNN), quasi-recurrent neural network (QRNN), skip recurrent neural network (SkipRNN), hierarchical multi-scale recurrent neural network (HM-RNN), fast-slow recurrent neural network (FS-RNN), recurrent highway network (RHN), autoregressive conditional heteroscedasticity (ARCH), generalized autoregressive conditional heteroscedasticity (GARCH) and extension GARCH (EGARCH )).","All recurrent models adopt GRU as their basic architecture, except that RHN which adopts LSTM, default activation function in hidden layers and output layers = Rectified Linear Unit (ReLU), default batch size = 64, optimiser=Adam, training mechanism=dropout gradient clipping + learning rate annealing","Dilated temporal convolutional network (TCN), dilated recurrent neural network (DilatedRNN), independently recurrent neural network (IndRNN), quasi-recurrent neural network (QRNN), skip recurrent neural network (SkipRNN), hierarchical multi-scale recurrent neural network (HM-RNN), fast-slow recurrent neural network (FS-RNN), recurrent highway network (RHN), autoregressive conditional heteroscedasticity (ARCH), generalized autoregressive conditional heteroscedasticity (GARCH) and extension GARCH (EGARCH )).",DilatedRNN and Dialated CNN ,Experimental simulation,Prediction accuracy,Average Negative Log-Likelihood (NLL).
2nd International Conference on Advances in Energy Engineering (ICAEE),Deep Learning for Time Series Forecasting: The Electric Load Case,2019,Switzerland,Europe,Power consumption and weather data,Sequential time series,electric load forecasting volatility problems ,A systematic experimental analysis of state of the art deep learning architectures for the short term electric load forecasting problems as a result of volatile data,"Feed Forward Neural Networks (FFNN) with architectural framework variants such as deep FFNN, Elmann Recurrent Neural Networks (ERNN) with architectural variants, Recurrent Neural Networks with architectural variants such as Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), Sequence To Sequence (seq2seq) architectures or encoder-decoder models and Convolutional Neural Networks (CNNs) with architectural variants Temporal Convolutional Neural Networks (TCN).",Implemented in Keras 2.12 with Tensorflow as backend and executed on a Linux cluster with an Intel(R) Xeon(R) Silver CPU and an Nvidia Titan XP.,"Feed Forward Neural Networks (FFNN) with architectural framework variants such as deep FFNN, Elmann Recurrent Neural Networks (ERNN) with architectural variants, Recurrent Neural Networks with architectural variants such as Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), Sequence To Sequence (seq2seq) architectures or encoder-decoder models and Convolutional Neural Networks (CNNs) with architectural variants Temporal Convolutional Neural Networks (TCN).",LSTM ,Comparative,Prediction accuracy,"Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Normalized Root Mean Squared Error and Proportion of Variance R2"
IEEE Communications Magazine,Deep Learning with Long Short-Term Memory for Time Series Prediction,2019,,,Trafic data,Sequential ,Learning long range dependencies problem in sequential time series data using deep learning algorithms,"To reduce considerable computing cost of deep Random Connectivity LSTM (RCLSTM) framework in traffic prediction and user-mobility forecasting. The experiments compared standard LSTM and RCLSTM frameworks with other three well-known sequential prediction framework Support Vector Regression (SVR), Autoregressive Integrated Moving Average (ARIMA) and Feed Forward Neural Networks (FFNNs) with sparse neural connections.",Random Connectivity LSTM (RCLSTM) framework,"The deep learning framework had a three-layer stack RCLSTM neural network with a memory cell size of 300 per layer. FFNN input features=100 and first and second hidden layer neurons=50. LSTM memory cell size= 30 with many trainable parameters, RCLSTM memory cell size=300",FFNN and  LSTM ,RCLSTM and LSTM,Comparative,Prediction accuracy and efficiency,Root Mean Square Error (RMSE)
International Research Journal of Engineering and Technology (IRJET) ,Automated Feature Selection and Churn Prediction using Deep Learning Models,2017,India,Asia,Mobile teleccomunnication ,Sequential  ,"Traditional churn prediction models such as Decision tree, Support vector machine and Neural Network had efficiency problems on different datasets","
Investigating the applications of deep learning frameworks in predicting the moment at which a telecommunication subscriber leave or switchfrom one service provider to the other (churn prediction).The design of an effective churn prediction model or framework is very dificult because of complex customer dataset in terms of features, volume and behaviour","Small Feedforward Neural Network (SFNN) , Large Feedforward Neural Network (LFNN) and  Convolutional Neural Network (CNN)","Model 1 = Small Feedforward Neural Network (SFNN) with total number of layers=3 with layer 1=input layer, layer 2= dense layer(Input dimension=74,Number of Neurons=37, Initialization=Normal and Activation=Relu) and layer 3, Layer3=Dense (Number of Neurons=1, Initialization=Normal and Activation=SigMoid); Model 2 = Large Feedforward Neural Network (LFNN) with total number of layers=4 with layer 1=input layer, layer 2=dense layer(Input dimension=74, Number of Neurons=74, Initialization=Normal and Activation=Relu) , Layer 3=Dense (Input dimension=74, Number of Neurons=37, Initialization=Normal and Activation=ReLu) and Layer4=Dense(Number of Neurons=1, Initialization=Normal and Activation=SigMoid); Model 3= Convolutional Neural Network (CNN) with Layer 1=Input, Layer 2=Embedding, Layer 3=Convolution1D, Layer 4=GlobalMaxPooling1D, Layer 5=Dense, Layer 7=Dropout and Layer 7=Dense)","Small Feedforward Neural Network (SFNN) ,  Large Feedforward Neural Network (LFNN) and  Convolutional Neural Network (CNN).",Large Feedforward Neural Network (LFNN),Experimental simulation,Prediction accuracy,Root Mean Square Error (RMSE)
"9th International Conference, BICS 2018 Xian China",Exploiting Deep Learning for Persian Sentiment Analysis,2019,UK,Europe,Natural launguage processing data from social Media ,Sequential ,Limited research for state of the art deep learning models for sentimental analysis of the Persian language,Developing and applying two (2) deep learning frameworks (deep autoencoders and deep convolutional neural networks (CNNs)) for sentiment analysis of a novel movie reviews dataset n Persian language. Performance of deep learning frameworks was compared with multilayer perceptron (MLP).,Deep autoencoders and deep convolutional neural networks (CNNs),"The Autoencoder consists of input, output and hidden layers. Autoencoder had input layer=1, three hidden layers=3 with (1500, 512, 1500) and output layer=1. Convolutional Neural Networks had 3 layers (input, hidden and output layer) with 11 layers: 4 convolution layers, 4 max pooling and 3 fully connected layers.",Deep autoencoders and deep convolutional neural networks (CNNs),deep convolutional neural networks (CNNs),Experimental simulation,Prediction accuracy,"Precision, Recall, f-Measure and Prediction accuracy"
Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence,Enhancing Stock Movement Prediction with Adversarial Training,2019,China,Asia,Financial stock price data ,Sequential,"As such, normal training with static price-based features (e.g., the close price) can easily over fit the data, being insufficient to obtain reliable models. Generalisation problem with predictive neural frameworks associated caused by the stochasticity of stock features in financial stock price datasets.",Demonstrate the effectiveness of adversarial training in enhancing the robustness and generalization of the deep Adversarial Attentive LSTM (Adv-ALSTM) framework in predicting the financial stock price movement rather stock price prediction.,"Momentum (MOM), MR Mean reversion (MR), LSTM ,  StockNet which uses a Variational Autoencoder (VAE) and Adv-ALSTM framework. Attentive LSTM (ALSTM) had four layers:
feature mapping layer, LSTM layer, temporal attention,
and prediction layer","Baselines methods includes MOM Momentum (MOM), MR Mean reversion (MR) and LSTM by (Nelson et al., 2017) with 3 tuned hyperparameters (number of hidden units (U), lag size (T), and weight of regularization term), Attentive LSTM (ALSTM) by (Qin et al., 2017) and StockNet uses a Variational Autoencoder (VAE). The Adv-ALSTM framework inherits the best optimal settings from ALSTM. These settings are hyperparameter tuned using Gridsearch selected through a grid-search method within the ranges of [4, 8, 16, 32], [2, 3, 4, 5, 10, 15], and [0.001, 0.01, 0.1, 1]. The Adv-ALSTM is then optimized using the mini-batch Adam by (Diederik and Jimmy, 2015), batch size=1,024 and initial learning rate=0.01.  Code web link = (https://github.com/hennande/Adv-ALSTM).","Momentum (MOM), MR Mean reversion (MR), LSTM ,  StockNet which uses a Variational Autoencoder (VAE) and Adv-ALSTM framework. Attentive LSTM (ALSTM) had four layers:
feature mapping layer, LSTM layer, temporal attention,
and prediction layer",Adv-ALSTM ,Experimental simulation,Prediction accuracy and Matthews Correlation Coefficient,"Accuracy (Acc) and Matthews Correlation Coefficient (MCC) by (Xu and Cohen, 2018)"
Joint European Conference on Machine Learning and Knowledge Discovery in Databases,Financial Series Prediction: Comparison between Precision of Time Series Models and Machine Learning Methods,2017,,,Financial stock price,Sequential ,Precise financial forecasting problem because of extreme sequential financial market data.,"Experimental precision forecasting comparison of sequential financial frameworks, traditional models and mainstream machine learning frameworks along with deep learning framework based on real time series stock price data sets.","Traditional time series frameworks (ARIMA), Mainstream machine learning frameworks (logistic regression (LR), multiple-layer perceptron (MLP) and support vector machine (SVM)) and Deep learning frameworks (Denoising Autoencoders (DAE)).",,"Traditional time series frameworks (ARIMA), Mainstream machine learning frameworks (logistic regression (LR), multiple-layer perceptron (MLP) and support vector machine (SVM)) and Deep learning frameworks (Denoising Autoencoders (DAE)).",Ensemble Denoising Autoencoders and Support Vector Machine,,Performance precision ,hit ratio
Joint European Conference on Machine Learning and Knowledge Discovery in Databases,Forecasting of Jump Arrivals in Stock Prices: New Attention-based Network Architecture using Limit Order Book Data,2017,,,Financial stock price,Sequential,return jump arrivals forecasting challenge.,"The work proposed a new neural network architecture based on Convolutional Long Short-Term Memory with Attention mechanism for predicting unstable return jump or spikes arrivals financial stock price. A nonparametric jump detection test algorithm designed by (Lee and Mykland, 2008) in mapping the distribution of jumps or spikes. The proposed convolutional LSTM attention framework utilized LSTM architecture for time series memory, convolution (CNN), and the attention model for reducing the input size, increasing locality, and focusing on the most critical features in the sequential dataset for better forecasting output.","Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM) , CNN LSTM-Attention (CNN-LSTM-A), Multi-Layer Perceptron Network (MLP), Random Classifier (Random) and CNN-LSTM using  time feature v10 (CNN-LSTM-v10)","Python Keras TensorFlow environment. MLP network (Activation function=2 leaky ReLu layers, neurons per layer=40), CNN network (layers=8, as designed by Tsantekidis et al. (2017b)), LSTM network (hidden neurons per layer =40, activation= Leaky ReLu) as presented in Tsantekidis et al. (2017b), CNN-LSTM-Attention network (first layer = attention layer, activation=tanh and softmax, LSTM layer neurons =40, Dense layer neurons =40, Dense layer neurons =1)","Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM) , CNN LSTM-Attention (CNN-LSTM-A), Multi-Layer Perceptron Network (MLP), Random Classifier (Random) and CNN-LSTM using no other information but the time feature v10 (CNN-LSTM-v10)",CNN-LSTM-Attention ,Experimental simulation,Performance Precision,"Precision Jumps= Recall, Precision=F1 score, agreement= Cohen's Kappa"
International Joint Conference on Neural Networks (IJCNN),High-Performance Stock Index Trading: Making Effective Use of a Deep Long Short-Term Memory Network,2019,Greece,Europe,"Financial stock indices dataset (S&P 500, Dow Jones Industrial Average (DJIA), NASDAQ and Russel 2000)",Sequential,,"To present a deep long short-term memory (LSTM) framework for accurate sequential asset price prediction (up/down) performance and profitable trading strategies. The constructed deep architecture was optimised in promoting profitability beyond just accuracy on 4 major US stock indices (S&P500, the DJIA, the NASDAQ and the Russel 2000) for over 8 years.",deep long short-term memory (LSTM) framework,"Training algorithm = back-propagation through time (BPTT), Optimiser = ADAM environment = Python 3 and TensorFlow v1.8. Network weights initialisation = uniform ribution (using the Glorot-Xavier uniform method), learning rate = exponential decay, number of iteration = 1600 iterations, hyperparameters tuning = grid-search with parameter ranges (number of LSTM layers (2; 3), number of units per LSTM layer (H) (32; 64; 128), the dropout parameter (0%, 50%, 70%) and length of the input sequence (T) (11; 22; 44)).",deep long short-term memory (LSTM) framework and ARIMA,deep long short-term memory (LSTM) framework,Experimental simulation,"Prediction performance based on accuracy, trading profitability measure and baseline comparison","Prediction performance based on accuracy ( Mean Directional Accuracy (MDA), Mean Squared Error (MSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE) And Correlation Coefficient (R2)), Trading profitability measures ( Cumulative Return (CR), Annualized Return (AR), Annualized Volatility (AV), Sharpe Ratio and (SR) and Draw-Down (DD)), Baseline comparison (Recent experimental studies by Bao et al. (2017), Zhou et al. (2019) Baek and Kim (2018), Sezer and Ozbayoglu (2018), Krauss et al. (2017) and Fischer and Krauss (2018) Chiang et al. (2016), Other models = ARIMA based on all 5 Error metrics profitability)"
International Conference on Data Mining Workshops (ICDMW) of 2019,Improved Forecasting of Cryptocurrency Price using Social Signals,2019,United States,North America,financial stock cryptocurrency and sentimental data from social media,Sequential ,,Financial price forecasting using neural network (LSTM) framework based on social signal from different social platforms in sequential speed and volatility cryptocurrency markets.,long short-term memory (LSTM)  and ARIMA,"Neural network architecture layer1= 400-dimensional LSTM layer, Layer2=800- dimensional LSTM layer, dense layer=single unit. Dense layer activation= 0 for regression task purposes, optimizer = ADAM. Parameter Tuning
Varied model parameters: batch sizes=16, 32, and 64; learning rates= 0.1, 0.01, 0.001, 0.0001, and 0.00001; LSTM layers ranging from 10 to 400-dimensional layers followed by 20 to 800-dimensional layers. 
",long short-term memory (LSTM) and ARIMA,LSTM ,Experimental simulation,Prediction performance based on accuracy and baseline comparison,"Performance error measurements: Root Mean Squared Error (RMSE), Mean Squared Percentage Error (MSPE), Mean Absolute Percentage Error (MAPE), Max Absolute Percentage Error (MaxAPE), and Root Mean Squared Percentage Error (RMSPE). "
"33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.",Deep Equilibrium Models,2019,Canada,North America,Natural language,sequential,The research solved hardware memory requirements limitation towards the development of high capacity deep networks during training.,The research proposed an optimum memory-efficient deep learning framework called the deep equilibrium model (DEQ) (https://github.com/locuslab/deq) in modelling sequential language datasets. The two feedforward DEQ derived from two state-of-the-art deep sequence models namely the self-attention transformers and trellis networks obtained state-of-the-art performance (SOTA) on various sequence tasks.,"Variational LSTM, Neural architecture search (NAS) Cell, NAS (w/ black-box hyperparameter tuner), AWD-LSTM, Differentiable architecture search (DARTS) (second order), DEQ-Transformer, Temporal Convolutional Networks (TCN), Gated Recurrent Unit (GRU), DEQ-TrellisNet and DEQ-Transformer.",DEQ-TrellisNet a weight-tied temporal convolutions and DEQ-Transformer a weight-tied self-attention mechanism architecture were efficiently implemented via quasi-Newton methods,"Variational LSTM, Neural architecture search (NAS) Cell, NAS (w/ black-box hyperparameter tuner), AWD-LSTM, Differentiable architecture search (DARTS) (second order), DEQ-Transformer, Temporal Convolutional Networks (TCN), Gated Recurrent Unit (GRU), DEQ-TrellisNet and DEQ-Transformer.",DEQ-TrellisNet and DEQ-Transformer,Experimental simulation,Hardware efficiency,Copy memory loss and Memory footprints
International Research Journal of Engineering and Technology (IRJET) ,Stock Market Prediction using Deep Learning and Sentiment Analysis,2019,India,Asia,stock price ,sequential ,Precise stock market prediction challenges using either sentimental analysis or stock trend analysis or deep learning as a result of random walk behaviour of a stock time series data.,"To increase sequential stock market accuracy forecasting using the combination of sentimental analysis, stock trend analysis and deep learning for increased investment yield. The paper examined the correlation between the sentiments and the stock prices towards stock market prediction using a deep learning framework. This was implemented in a phased approach with phase I dealing with sentiment analysis and stock trend analysis whilst phase II deals with prediction using deep learning model. ",Deep feed forward neural network (FFNN),"hidden layers=3, Number of neurons per layer=150, Training algorithms= backpropagation, network optimizer = stochastic gradient and optimizer = Adam",Deep feed forward neural network (FFNN),Deep feed forward neural network (FFNN) ,Phased experimental approach.,Prediction accuracy,"MSE, MAE, MAPE and Cosine Proximity"
26th International Conference on Computational Linguistics,Leveraging Financial News for Stock Trend Prediction with Attention-Based Recurrent Neural Network,2018,Canada,North America,Reuters and Bloomberg sentimental data and Standard & Poor's 500 dataset from ,Sequential,Time series predictions challenge due to noise and volatile features for accurate stock price prediction.,To propose a deep attention-based LSTM (At-LSTM) stock price prediction model using financial news titles for both Standard & Poor's 500 index and individual companies. ,"Support Vector Machines (SVM), Attention based LSTM (At-LSTM), At-LSTM without sentence encoder (Bag-At-LSTM), At-LSTM without the character level composition (WEB-At-LSTM), news abstract Ab-At-LSTM( Ab-At-LSTM), news document At-LSTM (Doc-At-LSTM), technical indicator At-LSTM (Tech-At-LSTM), convolutional neural network  LSTM (CNN-LSTM), event tuples input neural network (E-NN),  embedding input CNN (EB-CNN) and knowledge graph embedding input CNN (KGEB-CNN). ","Bidirectional-LSTM architecture was applied to encode the news text and capture the context information, self-attention mechanism were applied to distribute attention on most relative words, news and days.","Support Vector Machines (SVM), Attention based LSTM (At-LSTM), At-LSTM without sentence encoder (Bag-At-LSTM), At-LSTM without the character level composition (WEB-At-LSTM), news abstract Ab-At-LSTM( Ab-At-LSTM), news document At-LSTM (Doc-At-LSTM), technical indicator At-LSTM (Tech-At-LSTM), convolutional neural network  LSTM (CNN-LSTM), event tuples input neural network (E-NN),  embedding input CNN (EB-CNN) and knowledge graph embedding input CNN (KGEB-CNN). ",Attention based LSTM ,,Prediction accuracy,Percantage accuracy
IEEE Intelligent Transportation Systems Conference (ITSC),Motorway Traffic Flow Prediction using Advanced Deep Learning,2019,Australia,Australia,Australia traffic flow ,Sequential,,To propose an advanced hybrid spatiotemporal Hybrid CNN-LSTM sequential deep learning framework for predicting unstable time series traffic flow along motorways.,"Back-propagation Neuronal Networks (BPNN), CNN, LSTM and Hybrid CNN-LSTM prediction","Hybrid CNN-LSTM architecture parameters were batch size range= [20; 30; 40 ;::: 75;100], learning rate = 0:0003 and the weight of the regularisation=L2, optimiser=Adam, SGD and AdaGrad.","Back-propagation Neuronal Networks (BPNN), CNN, LSTM and Hybrid CNN-LSTM prediction",LSTM ,Experimental simulation,Prediction accuracy,"Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Symmetric Mean Absolute Percentage Error (SMAPE)."
Journal of Difference Equations and Applications,Neural networks for stock price prediction,2018,China,Asia,Financial stock  price dataset ,Sequential ,Extremely volatile nature of financial markets.,"To survey and compare the sequential predictive power of neural network models (Back-propagation neural networks (BPNN), radial basis neural networks (RBFNN), general regression neural network (GRNN), support vector machine regression (SVMR) and least squares support vector machine regression (LS-SVMR) ) on three unrelated stocks.","Back-propagation neural networks (BPNN), radial basis neural networks (RBFNN), general regression neural network (GRNN), support vector machine regression (SVMR) and least squares support vector machine regression (LS-SVMR).",,"Back-propagation neural networks (BPNN), radial basis neural networks (RBFNN), general regression neural network (GRNN), support vector machine regression (SVMR) and least squares support vector machine regression (LS-SVMR).","Back-propagation neural networks (BPNN), ",Experimental simulation,Prediction accuracy,MSE and MAPE
Proceedings of International Conference on Neural Information Processing Systems,Multimodal deep learning for short-term stock volatility prediction,2018,United Kingdom,Europe,Financial stock price and sentimental news data. ,Sequential ,Short term or daily stock price volatility prediction problem.,To demonstrate the effectiveness of a hierarchical deep learning architecture approach for short term (daily) volatility prediction based on combining natural language and sequential financial dataset versus mainstream deep models that rely only on historical stock price data,"Generalized Autoregressive Conditional Heteroscedasticity (GARCH) econometric model for volatility forecasting and Hierarchical Neural Network architecture (BiLSTM Att) based on (LSTM) with bidirectional (Bi), attention (Att) mechanism on the sentimental analysis and a concatenation layer.","Hierarchical Neural Network architecture optimisation technique = mini-batch SGD, optimiser= Adam optimizer, early stopping epochs = 8, hyper parameter tuning = grid search, concatenation layer units = 512, Bidirectional LSTM units= 1024.","Generalized Autoregressive Conditional Heteroscedasticity (GARCH) econometric model for volatility forecasting and Hierarchical Neural Network architecture (BiLSTM Att) based on (LSTM) with bidirectional (Bi), attention (Att) mechanism on the sentimental analysis and a concatenation layer.",BiLSTM Att ,Experimental simulation,Predicition accuracy,"Mean Squared Error (MSE), Mean Absolute Error (MAE) and Regression Coefficient(R2)"
Proceedings of the 35th International Conference on Machine Learning,Accurate Uncertainties for Deep Learning Using Calibrated Regression,2018,Sweden,Europe,UCI datasets ,Sequential time series,"Lack of accurate, reliable, and interpretable modern deep learning models for uncertainty estimation over continuous variables.",To propose a new and simpler procedure for calibrating Bayesian regression frameworks on grocery sales on time series forecasting. This novel procedure is inspired by Platt scaling for classification. ,Bayesian deep learning algorithm ,"Feedforward neural network had two layers of 128 hidden units, dropout rate=0.5, activation function= ReLU. Recurrent networks were based on GRU architecture with two stacked layers, dropout = 0.5.","Bayesian deep learning algorithm , Bayesian linear regression, Bayesian feedforward and recurrent Bayesian neural networks",calibrated Bayesian deep learning algorithm ,Experimental simulation,"Prediction accuracy, output consistency and visualizations and metrics for evaluating calibration and sharpness",Mean absolute percent errors (MAPE)
Proceeding of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17),Autoregressive Convolutional Recurrent Neural Network for Univariate and Multivariate Time Series Prediction,2019,Netherlands,Europe,"Univariate and multivariate (weather, energy and SML2010 ) datasets",Sequential time series ,"Time Series forecasting (univariate and multivariate) is a problem of high complexity due the different patterns that have to be detected in the input, ranging from high to low frequencies ones.","To propose a new sequential forecasting hybrid architecture of combining RNN and CNN that utilizes convolutional layers for feature extraction, a recurrent encoder and a linear autoregressive component. The model was tested and compared against known baseline architectures for univariate and multivariate time series analysis.",hybrid architecture of combining RNN and CNN ,"The had 3 parts; a multi-scale, convolutional part to input data feature extraction,  a recurrent part with 3 GRU units to encode the sequence, followed by a linear transformation to obtain the output and (c) an autoregressive part.","Auto-Regressive Integrated Moving Averages (ARIMA), Support Vector Machines (SVM), Simple LSTM, Deep GRU, SVM, ARIMA and the original LSTNet.",hybrid architecture of combining RNN and CNN ,Comparative experimental anysis,Prediction accuracy ,"Mean Squared Error (MSE), Mean Absolute Error (MAE) and Dynamic Time Warping (DTW)) using FastDTW"
3rd International Conference on Cloud Computing and Intelligence Systems 2014 ,Comparison of Deep Learning Models on Time Series Forecasting: A Case Study of Dissolved Oxygen Prediction,2019,United States,North America,Real time Yangtze River dissolved oxygen time series data ,sequential ,"There is no comprehensive comparison analysis of existing deep learning models for sequential learning, moreover most recent research are limited to just on one-step forecasting based on smaller datasets.",To compare and analyse the sequential prediction performance of state of the art deep learning models based on real time dissolved oxygen time series dataset.,"Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional LSTM (BiLSTM), Bidirectional GRU (BiGRU) and Temporal Convolutional Network (TCN)","Loss function = Mean Squared Error (MSE), optimiser= Adam, hyperparameters ( number of epochs of each model = 20, For LSTM, GRU, BiLSTM, and BiGRU, we set their units, batch size, validation split as 50, 128, and 0.1, respectively, For TCN, we set its dilations [1,2,4,8,16,32], with the kernel size= 3 and amount of filters= 4. For CNN, filters, kernel sizes, and validation split are 16, 3, 0, 1)","Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional LSTM (BiLSTM), Bidirectional GRU (BiGRU) and Temporal Convolutional Network (TCN)",Gated Recurrent Unit (GRU),Experimental simulation,Reliability and Prediction accuracy,"Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and coefficient of determination in each time step"
2017 IEEE International Conference on Data Mining Workshops (ICDMW),Deep and Confident Prediction for Time Series at Uber,2017,United States,North America,Uber ,Sequential time series ,"Accurate sequential forecasting challenge using existing uncertainty estimation models especially those with probabilistic formulation nature are hard to tune, scale and they add exogenous variables (other outside variable)",To propose a novel end-to-end deep Bayesian Neural Network (BNN) model for uncertainty estimation using sequential time series dataset. The BNN design combines LSTM encoder-decoder and a prediction network., Deep Bayesian Neural Network (BNN) model,"BNN architecture had 2 major components: an encoder-decoder framework that captures the inherent pattern in the time series with 2 layer LSTM cells, with 128 and 32 hidden states, respectively and a prediction network that takes input from both the learned embedding from encoder decoder with 3 fully connected layers with tanh activation, with 128, 64, and 16 hidden units, respectively.","Naive model Last-Day, quantile random forest (QRF), vanilla LSTM and our model (BNN) ",deep Bayesian Neural Network (BNN),Experimental simulation,prediction accuracy ,Symmetric Mean Absolute Percentage Error (SMAPE)
Proceeding of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17),Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction,2017,China,Asia,"Sequential taxicab GPS, meteorology and bike system data ",Sequential time series ,"Simultaneous forecasting the inflow and outflow of crowds in each region of a city is complex because of spatial dependencies, temporal dependencies and external influence factors.",To propose a deep sequential spatio-temporal residual neural network (ST-ResNet) framework for predicting crowd traffic inflow and outflow of crowds in Beijing city and New York City (NYC).,deep sequential spatio-temporal residual neural network (ST-ResNet) framework,"The ST-ResNet is trained via backpropagation, optimiser= Adam, final activation = tanh (between -1 and 1), normalization method= Min-Max, environment=python libraries (Theano and Keras). The convolutions of Conv1 and all residual units use 64 filters of size 3 _ 3, and Conv2 uses a convolution with 2 filters of size 3 _ 3 and  batch size =32.","Historical inflow and outflow (HA), ARIMA, Seasonal ARIMA (SARIMA), Vector Auto-Regressive (VAR), spatio-temporal model (ST-ANN), deep neural network (DNN)-based prediction model for spatio-temporal data (DeepST) with 4 variants, including temporal closeness sequence (DeepST-C), DeepST-C +periodic sequence (DeepSTCP), DeepSTCP + seasonal trend sequence (DeepST-CPT) and DeepST-CPT + meta data (DeepST-CPTM)",deep sequential spatio-temporal residual neural network (ST-ResNet),Experimental simulation,Prediction accuracy,Root Mean Square Error (RMSE)
"33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. ",DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News,2019,Canada,North America,Sequential S&P 500 stock price and sentimental news from financial domain. ,Sequential ,Sequential stock price dataset complexity which require extensive analysis resources.,To propose a novel deep differential privacy-inspired LSTM for stock prediction based on sentimental analysis. The designed DP-LSTM framework reduce prediction errors and increase model robustness.,deep differential privacy-inspired LSTM (DP- LSTM),"DP- LSTM network consists of 3 components: LSTM, VADER model and differential privacy (DP) mechanism, loss function =mean square error (MSE) and optimizer= ADAM","LSTM without news, LSTM with news and DP-LSTM",DP- LSTM,Experimental simulation,Prediction accuracy and robustness,"MSE, Mean Prediction Accuracy (MPA) and Mean Error Percent"
The 28th ACM International Conference on Information and Knowledge Management,DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting,2019,China,Asia,Daily gas revenue,Sequential time series ,Traditional models deficiency towards capturing of complex nonlinear or dynamic dependencies between time steps and between multiple time series,"To propose a Dual Self-Attention Network (DSANet) for highly efficient multivariate sequential time series forecasting, especially for dynamic-period or non-periodic series",Dual Self-Attention Network (DSANet) ,"Dual Self-Attention Network (DSANet) Optimisation= mini-batch stochastic gradient descent (SGD), optimiser= Adam optimizer, loss function= mean square error (MSE), environment= Python 3.6, library= PyTorch 1.0, hardware=Intel i7-8700 CPU, GTX1060 GPU, 6 cores with 32 GB RAM.","VAR, LRidge, LSVR, GP, GRU, LSTNet-S, LSTNet-A and TPA.",Dual Self-Attention Network (DSANet) ,Experimental simulation,Prediction accuracy and robustness,"Root Relative Squared Error (RRSE), Mean Absolute Error (MAE) and Empirical Correlation Coefficient (CORR) on different window {32, 64, 128} and horizon {3, 6, 12, 24} values for different forecasting tasks. Lower value is better for RRSE and MAE while a higher value is better for CORR "
Proceedings of the 28th ACM International Conference on Information and Knowledge Management,Machine learning vs statistical methods for time series forecasting: size matters,2019,Portugal,Europe,Benchmark time series dataset ,Sequential time series,Predictive performance challenges associated with existing models. As well as lack of well-established and explainable literature for sequential predictive machine learning methods as compared to statistic methods.,"To compare statistical methods with machine learning methods for time series forecasting, in presenting an empirical analysis of the impact of dataset sample size in the relative performance of different forecasting methods.","Statistical models ( ARIMA, Naive2, Theta, exponential smoothing state-space (ETS), exponential smoothing state space model with Box-Cox transformation, ARMA errors, trend and seasonal components (Tbats)) and Machine learning model (Rule-Based Regression (RBR), Random Forest (RF), Gaussian Process regression (GP) , multivariate adaptive regression splines (MARS) and Generalized Linear Regression Model (GLM))",Optimisation=Grid Search,"Statistical models ( ARIMA, Naive2, Theta, exponential smoothing state-space (ETS), exponential smoothing state space model with Box-Cox transformation, ARMA errors, trend and seasonal components (Tbats)) and Machine learning model (Rule-Based Regression (RBR), Random Forest (RF), Gaussian Process regression (GP) , multivariate adaptive regression splines (MARS) and Generalized Linear Regression Model (GLM))",Naive2,Experimental simulation,Computational complexity ,Computational time spent by a model
"Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, USA",MOrdReD: Memory-based Ordinal Regression Deep Neural Networks for Time Series Forecasting,2018,United Kingdom,Europe,Time series,Sequential time series,Robust and accuracy challenges associated with existing sequential forecasting ,To develop a state-of-the-art memory-based ordinal regression deep neural networks (MOrdReD) based on recurrent neural network framework probabilistic for long-term sequential time series prediction. It incorporate state-of-the-art developments from the Machine Learning and Natural Language Processing communities,Memory-Based Ordinal Regression Deep Neural Networks (MOrdReD),"MOrdReD models environment= Python 2.7 and using Keras 2.1.2 with the Tensorflow backend, optimisation =Adam, maximum epochs=50, encoder= bidirectional and Autoregressive GP environment=GPy 1.9.2, AR(p) environment=Python library Stats Models 0.9. Code is available https://www.github.com/bperezorozco/ordinal_tsf

","RNN regression, Autoregressive modelling of order p (AR(p)), Gaussian Process auto regression (GP)",Memory-Based Ordinal Regression Deep Neural Networks (MOrdReD),Experimental simulation,Prediction accuracy,"Symmetric Mean Absolute Percentage Error (SMAPE), RMSE) and NLL"
Journal of Advances in Modelling Earth Systems (JAMES),Analog forecasting of extreme-causing weather patterns using deep learning,2019,United States of America,North America,weather ,Sequential,"Numerical weather prediction (NWP) models require ever-growing computing time and resources, but still there is prediction accuracy challenges associated with environments which are characterised with unstable extreme weather patterns.  ",To propose a state of the art multi-variate data-driven deep learning (capsule neural networks) for accurate and fast extreme weather prediction framework. To also provide an early warnings system for extreme events focused on civil protection. The research focused on a novel pattern-recognition technique for modelling extreme surface temperature events over North America. ,Capsule Neural Network (CapsNet) ,"CapsNet had 2 CNN layers and ReLU layers, filters= 32 and 64. optimizer =ADAM. CapsNet and ConvNet codes available on GitHub: https://github.com/ashesh6810/DLC ","CNN, Logistic Regression, Capsule Neural Network",Capsule Neural Network (CapsNet) ,Experimental simulation,Prediction accuracy,Recalls
Neurocomputing,Probabilistic Forecasting with Temporal Convolutional Neural Network,2019,China,Asia,Real world industrial data and benchmark public datasets,Sequential time series ,Forecasting deficiencies in accurate capturing of uncertainity of the future.,To present a deep probabilistic forecasting framework based on Temporal Convolutional Neural Network (DeepTCN) for accurate learning of complex sequential patterns. The framework is supposed to learn latent correlation among series and handle complex real-world forecasting situations.,,DeepTCN consist of stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. ,"Seasonal ARIMA (SARIMA), Gradient boosting tree method (XGBoost), probabilistic method (JD-online), MatFact by Yu et al. (2016), DeepAR by Flunkert et al. (2017) and DeepState by Rangapuramet al. (2018)",,,Prediction accuracy and efficiency,"Symmetric Mean Absolute Percent Error (SMAPE), Root Mean Squared Logarithmic Error (RMLSE), Normalized Deviation (ND) and Normalized RMSE (NRMSE)"
2019 IEEE International Conference on Big Data (Big Data),Recurrent Neural Networks for Time Series Forecasting: Current Status and Future Directions,2019,Australia,Australia,Forecasting competitions,Sequential time series,Performance challenges associated with sequential neural network models,"To present an extensive empirical study of existing sequential recurrent neural network (RNN) frameworks for time series forecasting by implementing over 80 model architecture based on 3 RNN units Elman RNN cell, LSTM cell and the GRU and 3 learning algorithms optimisers (Adam , Adagrad  and COntinuous COin Betting (COCOB)). To also develop guidelines and best practices for their use. ",Recurrent neural network (RNN) architectures,"Application of different optimisers (Adam, Adagrad and COCOB) on different combinational architectures ","Statistical ETS, ARIMA,  Elman Recurrent Unit, Gated Recurrent Unit, Long Short-Term Memory , Sequence to Sequence 
Stacked ones and Hybrid ones with different optimizers (Adam, Adagrad and COCOB) 
",Recurrent neural network (RNN) architectures,Experimental simulation,Prediction accuracy,"Mean Symmetric Mean Absolute Percentage Error (SMAPE), Median SMAPE, Mean Mean Absolute Scaled Error (MASE), Median MASE, Rank SMAPE and Rank MASE "
Journal of Advanced Transportation,ST-LSTM: A Deep Learning Approach Combined Spatio-Temporal Features for Short-Term Forecast in Rail Transit,2019,China,Asia,Traffic,sequential,Solving irregular variations in sequential traffic datasets towards short-term traffic forecasting complexity.,To study and improve short-term traffic forecasting using a novel forecast model of combining spatio-temporal features based on spatio-temporal long short-term network (ST-LSTM).,Spatio-temporal long short-term network (ST-LSTM),"ST-LSTM Network. Based on LSTM network, a fully connected layer is added to combine temporal features and spatial features in ST-LSTM network. ","Seasonal ARIMA (SARIMA) [23], Support Vector Regression Model combined with Particle Swarm Optimization (PSO-SVR) [24], LSTM and the proposed ST-LSTM network",Spatio-temporal long short-term network (ST-LSTM),Experimental simulation,"Predicion ccuracy, stability and efficiency","ME, MAE, RMSE, MRE and Operation time"
